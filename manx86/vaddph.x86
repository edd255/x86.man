'\" t
.nh
.TH "X86-VADDPH" "7" "May 2019" "TTMO" "Intel x86-64 ISA Manual"
.SH NAME
VADDPH - ADD PACKED FP16 VALUES
.TS
allbox;
l l l l l 
l l l l l .
\fBInstruction En Bit Mode Flag Support Instruction En Bit Mode Flag Support 64/32 CPUID Feature Instruction En Bit Mode Flag CPUID Feature Instruction En Bit Mode Flag Op/ 64/32 CPUID Feature Instruction En Bit Mode Flag 64/32 CPUID Feature Instruction En Bit Mode Flag CPUID Feature Instruction En Bit Mode Flag Op/ 64/32 CPUID Feature\fP	\fB\fP	\fBSupport\fP	\fB\fP	\fBDescription\fP
T{
EVEX.128.NP.MAP5.W0 58 /r VADDPH xmm1{k1}{z}, xmm2, xmm3/m128/m16bcst
T}	A	V/V	AVX512-FP16 AVX512VL	T{
Add packed FP16 value from xmm3/m128/m16bcst to xmm2, and store result in xmm1 subject to writemask k1.
T}
T{
EVEX.256.NP.MAP5.W0 58 /r VADDPH ymm1{k1}{z}, ymm2, ymm3/m256/m16bcst
T}	A	V/V	AVX512-FP16 AVX512VL	T{
Add packed FP16 value from ymm3/m256/m16bcst to ymm2, and store result in ymm1 subject to writemask k1.
T}
T{
EVEX.512.NP.MAP5.W0 58 /r VADDPH zmm1{k1}{z}, zmm2, zmm3/m512/m16bcst {er}
T}	A	V/V	AVX512-FP16	T{
Add packed FP16 value from zmm3/m512/m16bcst to zmm2, and store result in zmm1 subject to writemask k1.
T}
.TE

.SH INSTRUCTION OPERAND ENCODING
.TS
allbox;
l l l l l l 
l l l l l l .
\fBOp/En\fP	\fBTuple\fP	\fBOperand 1\fP	\fBOperand 2\fP	\fBOperand 3\fP	\fBOperand 4\fP
A	Full	ModRM:reg (w)	VEX.vvvv (r)	ModRM:r/m (r)	N/A
.TE

.SS Description
This instruction adds packed FP16 values from source operands and stores
the packed FP16 result in the destination operand. The destination
elements are updated according to the writemask.

.SS Operation
.SS VADDPH (EVEX Encoded Versions) When SRC2 Operand is a Register
.EX
VL = 128, 256 or 512
KL := VL/16
IF (VL = 512) AND (EVEX.b = 1):
    SET_RM(EVEX.RC)
ELSE
    SET_RM(MXCSR.RC)
FOR j := 0 TO KL-1:
    IF k1[j] OR *no writemask*:
        DEST.fp16[j] := SRC1.fp16[j] + SRC2.fp16[j]
    ELSEIF *zeroing*:
        DEST.fp16[j] := 0
    // else dest.fp16[j] remains unchanged
DEST[MAXVL-1:VL] := 0
.EE

.SS VADDPH (EVEX Encoded Versions) When SRC2 Operand is a Memory Source
.EX
VL = 128, 256 or 512
KL := VL/16
FOR j := 0 TO KL-1:
    IF k1[j] OR *no writemask*:
        IF EVEX.b = 1:
            DEST.fp16[j] := SRC1.fp16[j] + SRC2.fp16[0]
        ELSE:
            DEST.fp16[j] := SRC1.fp16[j] + SRC2.fp16[j]
    ELSE IF *zeroing*:
        DEST.fp16[j] := 0
    // else dest.fp16[j] remains unchanged
DEST[MAXVL-1:VL] := 0
.EE

.SS Intel C/C++ Compiler Intrinsic Equivalent
.EX
VADDPH __m128h _mm_add_ph (__m128h a, __m128h b);

VADDPH __m128h _mm_mask_add_ph (__m128h src, __mmask8 k, __m128h a, __m128h b);

VADDPH __m128h _mm_maskz_add_ph (__mmask8 k, __m128h a, __m128h b);

VADDPH __m256h _mm256_add_ph (__m256h a, __m256h b);

VADDPH __m256h _mm256_mask_add_ph (__m256h src, __mmask16 k, __m256h a, __m256h b);

VADDPH __m256h _mm256_maskz_add_ph (__mmask16 k, __m256h a, __m256h b);

VADDPH __m512h _mm512_add_ph (__m512h a, __m512h b);

VADDPH __m512h _mm512_add_ph (__m512h a, __m512h b);

VADDPH __m512h _mm512_mask_add_ph (__m512h src, __mmask32 k, __m512h a, __m512h b);

VADDPH __m512h _mm512_maskz_add_ph (__mmask32 k, __m512h a, __m512h b);

VADDPH __m512h _mm512_add_round_ph (__m512h a, __m512h b, int rounding);

VADDPH __m512h _mm512_mask_add_round_ph (__m512h src, __mmask32 k, __m512h a, __m512h b, int rounding);

VADDPH __m512h _mm512_maskz_add_round_ph (__mmask32 k, __m512h a, __m512h b, int rounding);
.EE

.SS SIMD Floating-Point Exceptions
Invalid, Underflow, Overflow, Precision, Denormal.

.SS Other Exceptions
EVEX-encoded instructions, see Table
2-46, “Type E2 Class Exception Conditions.”

.SH SEE ALSO
x86-manpages(7) for a list of other x86-64 man pages.

.SH COLOPHON
This UNOFFICIAL, mechanically-separated, non-verified reference is
provided for convenience, but it may be
incomplete or
broken in various obvious or non-obvious ways.
Refer to Intel® 64 and IA-32 Architectures Software Developer’s Manual
for anything serious.

.br
This page is generated by scripts; therefore may contain visual or semantical bugs. Please report them (or better, fix them) on https://github.com/ttmo-O/x86-manpages.

.br
MIT licensed by TTMO 2025 (Turkish Unofficial Chamber of Reverse Engineers - https://ttmo.re).
