'\" t
.nh
.TH "X86-VPSHLD" "7" "May 2019" "TTMO" "Intel x86-64 ISA Manual"
.SH NAME
VPSHLD - CONCATENATE AND SHIFT PACKED DATA LEFT LOGICAL
.TS
allbox;
l l l l l 
l l l l l .
\fBOpcode/Instruction\fP	\fBOp/En\fP	\fB64/32 bit Mode Support\fP	\fBCPUID Feature Flag\fP	\fBDescription\fP
T{
EVEX.128.66.0F3A.W1 70 /r /ib VPSHLDW xmm1{k1}{z}, xmm2, xmm3/m128, imm8
T}	A	V/V	AVX512_VBMI2 AVX512VL	T{
Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into xmm1.
T}
T{
EVEX.256.66.0F3A.W1 70 /r /ib VPSHLDW ymm1{k1}{z}, ymm2, ymm3/m256, imm8
T}	A	V/V	AVX512_VBMI2 AVX512VL	T{
Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into ymm1.
T}
T{
EVEX.512.66.0F3A.W1 70 /r /ib VPSHLDW zmm1{k1}{z}, zmm2, zmm3/m512, imm8
T}	A	V/V	AVX512_VBMI2	T{
Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into zmm1.
T}
T{
EVEX.128.66.0F3A.W0 71 /r /ib VPSHLDD xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst, imm8
T}	B	V/V	AVX512_VBMI2 AVX512VL	T{
Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into xmm1.
T}
T{
EVEX.256.66.0F3A.W0 71 /r /ib VPSHLDD ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst, imm8
T}	B	V/V	AVX512_VBMI2 AVX512VL	T{
Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into ymm1.
T}
T{
EVEX.512.66.0F3A.W0 71 /r /ib VPSHLDD zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst, imm8
T}	B	V/V	AVX512_VBMI2	T{
Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into zmm1.
T}
T{
EVEX.128.66.0F3A.W1 71 /r /ib VPSHLDQ xmm1{k1}{z}, xmm2, xmm3/m128/m64bcst, imm8
T}	B	V/V	AVX512_VBMI2 AVX512VL	T{
Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into xmm1.
T}
T{
EVEX.256.66.0F3A.W1 71 /r /ib VPSHLDQ ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst, imm8
T}	B	V/V	AVX512_VBMI2 AVX512VL	T{
Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into ymm1.
T}
T{
EVEX.512.66.0F3A.W1 71 /r /ib VPSHLDQ zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst, imm8
T}	B	V/V	AVX512_VBMI2	T{
Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into zmm1.
T}
.TE

.SH INSTRUCTION OPERAND ENCODING
.TS
allbox;
l l l l l l 
l l l l l l .
\fBOp/En\fP	\fBTuple\fP	\fBOperand 1\fP	\fBOperand 2\fP	\fBOperand 3\fP	\fBOperand 4\fP
A	Full Mem	ModRM:reg (w)	EVEX.vvvv (r)	ModRM:r/m (r)	imm8 (r)
B	Full	ModRM:reg (w)	EVEX.vvvv (r)	ModRM:r/m (r)	imm8 (r)
.TE

.SS Description
Concatenate packed data, extract result shifted to the left by constant
value.

.PP
This instruction supports memory fault suppression.

.SS Operation
.SS VPSHLDW DEST, SRC2, SRC3, imm8
.EX
(KL, VL) = (8, 128), (16, 256), (32, 512)
FOR j := 0 TO KL-1:
    IF MaskBit(j) OR *no writemask*:
        tmp := concat(SRC2.word[j], SRC3.word[j]) << (imm8 & 15)
        DEST.word[j] := tmp.word[1]
    ELSE IF *zeroing*:
        DEST.word[j] := 0
    *ELSE DEST.word[j] remains unchanged*
DEST[MAX_VL-1:VL] := 0
.EE

.SS VPSHLDD DEST, SRC2, SRC3, imm8
.EX
(KL, VL) = (4, 128), (8, 256), (16, 512)
FOR j := 0 TO KL-1:
    IF SRC3 is broadcast memop:
        tsrc3 := SRC3.dword[0]
    ELSE:
        tsrc3 := SRC3.dword[j]
    IF MaskBit(j) OR *no writemask*:
        tmp := concat(SRC2.dword[j], tsrc3) << (imm8 & 31)
        DEST.dword[j] := tmp.dword[1]
    ELSE IF *zeroing*:
        DEST.dword[j] := 0
    *ELSE DEST.dword[j] remains unchanged*
DEST[MAX_VL-1:VL] := 0
.EE

.SS VPSHLDQ DEST, SRC2, SRC3, imm8
.EX
(KL, VL) = (2, 128), (4, 256), (8, 512)
FOR j := 0 TO KL-1:
    IF SRC3 is broadcast memop:
        tsrc3 := SRC3.qword[0]
    ELSE:
        tsrc3 := SRC3.qword[j]
    IF MaskBit(j) OR *no writemask*:
        tmp := concat(SRC2.qword[j], tsrc3) << (imm8 & 63)
        DEST.qword[j] := tmp.qword[1]
    ELSE IF *zeroing*:
        DEST.qword[j] := 0
    *ELSE DEST.qword[j] remains unchanged*
DEST[MAX_VL-1:VL] := 0
.EE

.SS Intel C/C++ Compiler Intrinsic Equivalent
.EX
VPSHLDD __m128i _mm_shldi_epi32(__m128i, __m128i, int);

VPSHLDD __m128i _mm_mask_shldi_epi32(__m128i, __mmask8, __m128i, __m128i, int);

VPSHLDD __m128i _mm_maskz_shldi_epi32(__mmask8, __m128i, __m128i, int);

VPSHLDD __m256i _mm256_shldi_epi32(__m256i, __m256i, int);

VPSHLDD __m256i _mm256_mask_shldi_epi32(__m256i, __mmask8, __m256i, __m256i, int);

VPSHLDD __m256i _mm256_maskz_shldi_epi32(__mmask8, __m256i, __m256i, int);

VPSHLDD __m512i _mm512_shldi_epi32(__m512i, __m512i, int);

VPSHLDD __m512i _mm512_mask_shldi_epi32(__m512i, __mmask16, __m512i, __m512i, int);

VPSHLDD __m512i _mm512_maskz_shldi_epi32(__mmask16, __m512i, __m512i, int);

VPSHLDQ __m128i _mm_shldi_epi64(__m128i, __m128i, int);

VPSHLDQ __m128i _mm_mask_shldi_epi64(__m128i, __mmask8, __m128i, __m128i, int);

VPSHLDQ __m128i _mm_maskz_shldi_epi64(__mmask8, __m128i, __m128i, int);

VPSHLDQ __m256i _mm256_shldi_epi64(__m256i, __m256i, int);

VPSHLDQ __m256i _mm256_mask_shldi_epi64(__m256i, __mmask8, __m256i, __m256i, int);

VPSHLDQ __m256i _mm256_maskz_shldi_epi64(__mmask8, __m256i, __m256i, int);

VPSHLDQ __m512i _mm512_shldi_epi64(__m512i, __m512i, int);

VPSHLDQ __m512i _mm512_mask_shldi_epi64(__m512i, __mmask8, __m512i, __m512i, int);

VPSHLDQ __m512i _mm512_maskz_shldi_epi64(__mmask8, __m512i, __m512i, int);

VPSHLDW __m128i _mm_shldi_epi16(__m128i, __m128i, int);

VPSHLDW __m128i _mm_mask_shldi_epi16(__m128i, __mmask8, __m128i, __m128i, int);

VPSHLDW __m128i _mm_maskz_shldi_epi16(__mmask8, __m128i, __m128i, int);

VPSHLDW __m256i _mm256_shldi_epi16(__m256i, __m256i, int);

VPSHLDW __m256i _mm256_mask_shldi_epi16(__m256i, __mmask16, __m256i, __m256i, int);

VPSHLDW __m256i _mm256_maskz_shldi_epi16(__mmask16, __m256i, __m256i, int);

VPSHLDW __m512i _mm512_shldi_epi16(__m512i, __m512i, int);

VPSHLDW __m512i _mm512_mask_shldi_epi16(__m512i, __mmask32, __m512i, __m512i, int);

VPSHLDW __m512i _mm512_maskz_shldi_epi16(__mmask32, __m512i, __m512i, int);
.EE

.SS SIMD Floating-Point Exceptions
None.

.SS Other Exceptions
See Table 2-49, “Type E4 Class
Exception Conditions.”

.SH SEE ALSO
x86-manpages(7) for a list of other x86-64 man pages.

.SH COLOPHON
This UNOFFICIAL, mechanically-separated, non-verified reference is
provided for convenience, but it may be
incomplete or
broken in various obvious or non-obvious ways.
Refer to Intel® 64 and IA-32 Architectures Software Developer’s Manual
for anything serious.

.br
This page is generated by scripts; therefore may contain visual or semantical bugs. Please report them (or better, fix them) on https://github.com/ttmo-O/x86-manpages.

.br
MIT licensed by TTMO 2025 (Turkish Unofficial Chamber of Reverse Engineers - https://ttmo.re).
